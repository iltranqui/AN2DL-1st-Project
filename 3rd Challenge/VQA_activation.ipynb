{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Basic_Model_VQA_2_CORRECT.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9-final"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfDYBEE9UTbv"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J8gufz1UTb9"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import shutil\n",
        "\n",
        "# Set the seed for random operations. \n",
        "# This let our experiments to be reproducible. \n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Get current working directory\n",
        "cwd = os.getcwd()\n",
        "\n",
        "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t74cL6CTVDLX",
        "outputId": "75fe7e1f-96f7-45c2-bcf5-86517be9c499"
      },
      "source": [
        "cwd = os.getcwd()\n",
        "cwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\enric\\\\Downloads\\\\AN2DL-1st-Project\\\\3rd Challenge'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGmpHrx1VHA4",
        "outputId": "b381cf12-bdc5-4dbb-db0e-c2349a48109f"
      },
      "source": [
        "dataset_dir = os.path.join(cwd, 'VQA_Dataset')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1JRUORzUTcC"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPIPytlFUTcD"
      },
      "source": [
        "img_w = 700\n",
        "img_h = 400\n",
        "batch_size = 8\n",
        "lr = 2e-3\n",
        "\n",
        "MAX_NUM_WORDS = 5000 # max number of unique words in dictionary\n",
        "\n",
        "FEATURES = 2048 # size of feature vector for images and questions\n",
        "\n",
        "UNITS = 512\n",
        "\n",
        "PERC_DROP = 0.35\n",
        "\n",
        "decay = 0.5\n",
        "minimum = 1e-3"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bpL7Lpmv6e_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIv0si0qUTcE"
      },
      "source": [
        "labels_dict = {\n",
        "        '0': 0,\n",
        "        '1': 1,\n",
        "        '2': 2,\n",
        "        '3': 3,\n",
        "        '4': 4,\n",
        "        '5': 5,\n",
        "        'apple': 6,\n",
        "        'baseball': 7,\n",
        "        'bench': 8,\n",
        "        'bike': 9,\n",
        "        'bird': 10,\n",
        "        'black': 11,\n",
        "        'blanket': 12,\n",
        "        'blue': 13,\n",
        "        'bone': 14,\n",
        "        'book': 15,\n",
        "        'boy': 16,\n",
        "        'brown': 17,\n",
        "        'cat': 18,\n",
        "        'chair': 19,\n",
        "        'couch': 20,\n",
        "        'dog': 21,\n",
        "        'floor': 22,\n",
        "        'food': 23,\n",
        "        'football': 24,\n",
        "        'girl': 25,\n",
        "        'grass': 26,\n",
        "        'gray': 27,\n",
        "        'green': 28,\n",
        "        'left': 29,\n",
        "        'log': 30,\n",
        "        'man': 31,\n",
        "        'monkey bars': 32,\n",
        "        'no': 33,\n",
        "        'nothing': 34,\n",
        "        'orange': 35,\n",
        "        'pie': 36,\n",
        "        'plant': 37,\n",
        "        'playing': 38,\n",
        "        'red': 39,\n",
        "        'right': 40,\n",
        "        'rug': 41,\n",
        "        'sandbox': 42,\n",
        "        'sitting': 43,\n",
        "        'sleeping': 44,\n",
        "        'soccer': 45,\n",
        "        'squirrel': 46,\n",
        "        'standing': 47,\n",
        "        'stool': 48,\n",
        "        'sunny': 49,\n",
        "        'table': 50,\n",
        "        'tree': 51,\n",
        "        'watermelon': 52,\n",
        "        'white': 53,\n",
        "        'wine': 54,\n",
        "        'woman': 55,\n",
        "        'yellow': 56,\n",
        "        'yes': 57\n",
        "}\n",
        "\n",
        "num_answers = len(labels_dict)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOpOSZc_UTcF"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQapmFzNUTcF"
      },
      "source": [
        "import json\n",
        "def unwrap_weighted(path, split = 0.2):\n",
        "    \n",
        "    dataset_dir = os.path.join(path, 'train_questions_annotations.json')\n",
        "    training_dir = os.path.join(path, 'training.json')\n",
        "    validation_dir = os.path.join(path, 'validation.json')\n",
        "        \n",
        "    dic_images = None\n",
        "    \n",
        "    with open(dataset_dir) as f:\n",
        "       dic_images = json.load(f)\n",
        "        \n",
        "    dict_keys = list(dic_images.keys())\n",
        "    np.random.shuffle(dict_keys)\n",
        "    questions = int(round(split*len(dict_keys)))\n",
        "        \n",
        "    dic_validations = { dict_keys[i]:dic_images[dict_keys[i]] for i in range(questions)}\n",
        "    dic_training = {dict_keys[i]:dic_images[dict_keys[i]] for i in range(questions, len(dict_keys))}\n",
        "        \n",
        "    with open(training_dir, 'w') as fp:\n",
        "       json.dump(dic_training, fp)\n",
        "    with open(validation_dir, 'w') as fp:\n",
        "       json.dump(dic_validations, fp)\n",
        "\n",
        "path = os.getcwd()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUFnliG7UTcG"
      },
      "source": [
        "def get_token_dic_quest(path, max_num_words = 5000):\n",
        "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "    dataset_dir = os.path.join(path, 'train_questions_annotations.json')\n",
        "    \n",
        "    # Load dataset\n",
        "    with open(dataset_dir) as f:\n",
        "        dic_images = json.load(f)\n",
        "\n",
        "    # Get all questions as strings in a list\n",
        "    questions = [dic['question'] for dic in dic_images.values()]\n",
        "\n",
        "    # Strip '?' from questions\n",
        "    questions = [s.translate(str.maketrans('', '', '?')).lower() for s in questions if not s == '']\n",
        "    questions_tokenizer = Tokenizer(num_words=max_num_words)\n",
        "    questions_tokenizer.fit_on_texts(questions)\n",
        "\n",
        "    questions_wtoi = questions_tokenizer.word_index # index 0 reserved for padding\n",
        "    \n",
        "    questions_tokenized = questions_tokenizer.texts_to_sequences(questions)\n",
        "    max_question_length = max(len(sentence) for sentence in questions_tokenized)\n",
        "    \n",
        "    return questions_tokenizer, questions_wtoi, max_question_length\n",
        "\n",
        "\n",
        "def from_questions_to_dict(path, dict_req, max_num_words = 5000):\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    \n",
        "    # Return dictionary q_wtoi\n",
        "    tokenizer, wtoi, max_len = get_token_dic_quest(path, max_num_words = 5000)\n",
        "    \n",
        "    translated_dics = []\n",
        "    \n",
        "    for dic in dict_req:\n",
        "        \n",
        "        question = dic['question'].translate(str.maketrans('', '', '?')).lower()\n",
        "        question = tokenizer.texts_to_sequences([question])\n",
        "        question = pad_sequences(question, maxlen=max_len)\n",
        "        dic['question'] = question[0]\n",
        "        dic['answer'] = labels_dict[dic['answer']]\n",
        "        translated_dics.append(dic)\n",
        "    \n",
        "    return translated_dics"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bysPOHz0UTcI"
      },
      "source": [
        "from PIL import Image\n",
        "    \n",
        "# Patches Generator\n",
        "class dataset_generator(tf.keras.utils.Sequence):\n",
        "\n",
        "  def __init__(self, path, preprocessing, subset = \"training\", image_generator = None, batch_size = 5, max_num_words=5000):\n",
        "    json_file = subset + \".json\"\n",
        "    dat_dir = os.path.join(path, 'VQA_Dataset')\n",
        "    subset_file = os.path.join(dat_dir, json_file)\n",
        "    \n",
        "    with open(subset_file) as f:\n",
        "       dictionaries = json.load(f)\n",
        "       dictionaries = dictionaries.values()\n",
        "       self.dictionary = from_questions_to_dict(dat_dir, dictionaries, max_num_words)\n",
        "    \n",
        "    self.batch_size = batch_size\n",
        "    self.image_generator = image_generator\n",
        "    self.preprocessing = preprocessing\n",
        "    self.dat_dir = dat_dir\n",
        "    self.gen = image_generator\n",
        "    self.batch_size = batch_size\n",
        "    self.max_num_words = max_num_words\n",
        "    self.n = 0\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.dictionary)//self.batch_size\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    lower_bound = index*self.batch_size\n",
        "    upper_bound = (index+1)*self.batch_size\n",
        "    \n",
        "    batch_img = []\n",
        "    batch_que = []\n",
        "    batch_ans = []\n",
        "    \n",
        "    for idx in range(lower_bound, upper_bound):\n",
        "        img, que, ans = self.__data_generation__(idx)\n",
        "        batch_img.append(img)\n",
        "        batch_que.append(que)\n",
        "        batch_ans.append(ans)\n",
        "        \n",
        "    batch_img = np.stack(batch_img, axis=0)\n",
        "    batch_que = np.stack(batch_que, axis=0)\n",
        "    batch_ans = np.stack(batch_ans, axis=0)\n",
        "    \n",
        "    x = [batch_img, batch_que]\n",
        "    y = batch_ans\n",
        "    \n",
        "    return x, y\n",
        "    \n",
        "    \n",
        "  def __data_generation__(self, idx):\n",
        "    actual_dict = self.dictionary[idx]\n",
        "    \n",
        "    img_name = actual_dict['image_id']\n",
        "    answer = actual_dict['answer']\n",
        "    question = actual_dict['question']\n",
        "    \n",
        "    actual_img = Image.open(os.path.join(self.dat_dir, \"Images\", img_name + \".png\"))\n",
        "    actual_img = actual_img.convert('RGB')\n",
        "    img_arr = np.array(actual_img)\n",
        "    img_arr = np.expand_dims(img_arr, axis=0)\n",
        "    \n",
        "    if self.image_generator is not None:\n",
        "        img_arr = self.gen.random_transform(img_arr)\n",
        "    \n",
        "    if self.preprocessing is not None:\n",
        "        img_arr = self.preprocessing(img_arr)\n",
        "        \n",
        "    img_arr = np.squeeze(img_arr, axis=0)\n",
        "    \n",
        "    return img_arr, question, answer"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXPpjZmNUTcJ"
      },
      "source": [
        "Datasets generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCpK8iuxUTcL"
      },
      "source": [
        "unwrap_weighted(os.path.join(path, 'VQA_Dataset'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "taHaGZsHUTcL",
        "outputId": "995c951b-05c5-4e52-b961-7dedb2fbaf73"
      },
      "source": [
        "preprocessing_function = tf.keras.applications.resnet_v2.preprocess_input\n",
        "\n",
        "gen = dataset_generator(path = os.getcwd(), preprocessing = preprocessing_function, \n",
        "                  subset = \"training\", image_generator = None, max_num_words=5000, batch_size = batch_size)\n",
        "\n",
        "gen_val = dataset_generator(path = os.getcwd(), preprocessing = preprocessing_function, \n",
        "                  subset = \"validation\", image_generator = None, max_num_words=5000, batch_size = batch_size)\n",
        "\n",
        "'''\n",
        "dataset = tf.data.Dataset.from_generator(lambda: gen, output_types=([tf.float32, tf.uint8], tf.uint8), \n",
        "                                         output_shapes=([2,], ()))\n",
        "\n",
        "dataset_val = tf.data.Dataset.from_generator(lambda: gen_val, output_types=([tf.float32, tf.uint8], tf.uint8), \n",
        "                                         output_shapes=([2,], ()))\n",
        "\n",
        "dataset = dataset.batch(batch_size)\n",
        "dataset = dataset.repeat()\n",
        "\n",
        "dataset_val = dataset_val.batch(batch_size)\n",
        "dataset_val = dataset_val.repeat()\n",
        "\n",
        "iterator = iter(dataset)\n",
        "giggino = next(iterator)\n",
        "print(giggino)\n",
        "'''\n",
        "\n",
        "for f in gen:\n",
        "    print(f)\n",
        "    break\n",
        "    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndataset = tf.data.Dataset.from_generator(lambda: gen, output_types=([tf.float32, tf.uint8], tf.uint8), \\n                                         output_shapes=([2,], ()))\\n\\ndataset_val = tf.data.Dataset.from_generator(lambda: gen_val, output_types=([tf.float32, tf.uint8], tf.uint8), \\n                                         output_shapes=([2,], ()))\\n\\ndataset = dataset.batch(batch_size)\\ndataset = dataset.repeat()\\n\\ndataset_val = dataset_val.batch(batch_size)\\ndataset_val = dataset_val.repeat()\\n\\niterator = iter(dataset)\\ngiggino = next(iterator)\\nprint(giggino)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([array([[[[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         ...,\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ]],\n\n        [[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         ...,\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ]],\n\n        [[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         ...,\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ]],\n\n        ...,\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]],\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]],\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]]],\n\n\n       [[[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.4954191 ,  2.3235295 ,  2.5877128 ],\n         ...,\n         [-1.4842881 , -1.0728291 , -1.5081482 ],\n         [-1.5014129 , -1.107843  , -1.5255773 ],\n         [-1.5185376 , -1.1428571 , -1.5255773 ]],\n\n        [[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         ...,\n         [-1.5185376 , -1.160364  , -1.5255773 ],\n         [-1.5185376 , -1.160364  , -1.5255773 ],\n         [-1.5185376 , -1.160364  , -1.5255773 ]],\n\n        [[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         ...,\n         [-1.5185376 , -1.160364  , -1.5255773 ],\n         [-1.5185376 , -1.160364  , -1.5255773 ],\n         [-1.5185376 , -1.160364  , -1.5255773 ]],\n\n        ...,\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]],\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]],\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]]],\n\n\n       [[[ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         ...,\n         [ 1.7694151 ,  1.6407562 ,  1.4199566 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4199566 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4199566 ]],\n\n        [[ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         ...,\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ]],\n\n        [[ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         ...,\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ]],\n\n        ...,\n\n        [[ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         ...,\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292]],\n\n        [[ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         ...,\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292]],\n\n        [[ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         ...,\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292]]],\n\n\n       ...,\n\n\n       [[[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         ...,\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ]],\n\n        [[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         ...,\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ]],\n\n        [[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         ...,\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ]],\n\n        ...,\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]],\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]],\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]]],\n\n\n       [[[ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         ...,\n         [ 1.7694151 ,  1.6407562 ,  1.4199566 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4199566 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4199566 ]],\n\n        [[ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         ...,\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ]],\n\n        [[ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         [ 2.2489083 ,  2.1834733 ,  1.9776908 ],\n         ...,\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ],\n         [ 1.7694151 ,  1.6407562 ,  1.4025275 ]],\n\n        ...,\n\n        [[ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         ...,\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292]],\n\n        [[ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         ...,\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292]],\n\n        [[ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         ...,\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292],\n         [ 1.0673003 ,  0.69537824,  0.19991292]]],\n\n\n       [[[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         ...,\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ]],\n\n        [[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         ...,\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ]],\n\n        [[ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         ...,\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ],\n         [ 1.5296686 ,  2.3585434 ,  2.622571  ]],\n\n        ...,\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]],\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]],\n\n        [[-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         ...,\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ],\n         [-0.9876702 ,  0.13515405, -1.7347276 ]]]], dtype=float32), array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   2,   1,  16, 219,  13,   1, 288],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   5,   6, 141,   3,   8,  21, 132],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   5,   6, 172],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   2,   1, 169, 190, 121,   1, 103],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2,\n          1,  15, 104,  12, 512, 137,   1,  60],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   3, 117, 105,  22, 355],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   2,   1,  16,  84],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2,   1,  51,\n         34, 101,  12, 151,  27,   1,  54,  16]])], array([33,  4,  1, 33, 33, 57, 33, 33]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9ACdnU_UTcY"
      },
      "source": [
        "# Image Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s2cZyjvUTcb",
        "outputId": "fab5605b-354b-4869-e22c-198cc18410bf",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "image_encoder = tf.keras.applications.InceptionResNetV2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(img_h, img_w, 3),\n",
        "    pooling='avg'\n",
        ")\n",
        "\n",
        "for layer in image_encoder.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "image_encoder.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block20_0_relu (Activatio (None, 12, 21, 1248) 0           conv5_block20_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block20_1_conv (Conv2D)   (None, 12, 21, 128)  159744      conv5_block20_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block20_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block20_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block20_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block20_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block20_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block20_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block20_concat (Concatena (None, 12, 21, 1280) 0           conv5_block19_concat[0][0]       \n                                                                 conv5_block20_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block21_0_bn (BatchNormal (None, 12, 21, 1280) 5120        conv5_block20_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block21_0_relu (Activatio (None, 12, 21, 1280) 0           conv5_block21_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block21_1_conv (Conv2D)   (None, 12, 21, 128)  163840      conv5_block21_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block21_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block21_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block21_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block21_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block21_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block21_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block21_concat (Concatena (None, 12, 21, 1312) 0           conv5_block20_concat[0][0]       \n                                                                 conv5_block21_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block22_0_bn (BatchNormal (None, 12, 21, 1312) 5248        conv5_block21_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block22_0_relu (Activatio (None, 12, 21, 1312) 0           conv5_block22_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block22_1_conv (Conv2D)   (None, 12, 21, 128)  167936      conv5_block22_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block22_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block22_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block22_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block22_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block22_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block22_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block22_concat (Concatena (None, 12, 21, 1344) 0           conv5_block21_concat[0][0]       \n                                                                 conv5_block22_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block23_0_bn (BatchNormal (None, 12, 21, 1344) 5376        conv5_block22_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block23_0_relu (Activatio (None, 12, 21, 1344) 0           conv5_block23_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block23_1_conv (Conv2D)   (None, 12, 21, 128)  172032      conv5_block23_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block23_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block23_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block23_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block23_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block23_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block23_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block23_concat (Concatena (None, 12, 21, 1376) 0           conv5_block22_concat[0][0]       \n                                                                 conv5_block23_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block24_0_bn (BatchNormal (None, 12, 21, 1376) 5504        conv5_block23_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block24_0_relu (Activatio (None, 12, 21, 1376) 0           conv5_block24_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block24_1_conv (Conv2D)   (None, 12, 21, 128)  176128      conv5_block24_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block24_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block24_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block24_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block24_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block24_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block24_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block24_concat (Concatena (None, 12, 21, 1408) 0           conv5_block23_concat[0][0]       \n                                                                 conv5_block24_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block25_0_bn (BatchNormal (None, 12, 21, 1408) 5632        conv5_block24_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block25_0_relu (Activatio (None, 12, 21, 1408) 0           conv5_block25_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block25_1_conv (Conv2D)   (None, 12, 21, 128)  180224      conv5_block25_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block25_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block25_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block25_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block25_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block25_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block25_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block25_concat (Concatena (None, 12, 21, 1440) 0           conv5_block24_concat[0][0]       \n                                                                 conv5_block25_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block26_0_bn (BatchNormal (None, 12, 21, 1440) 5760        conv5_block25_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block26_0_relu (Activatio (None, 12, 21, 1440) 0           conv5_block26_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block26_1_conv (Conv2D)   (None, 12, 21, 128)  184320      conv5_block26_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block26_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block26_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block26_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block26_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block26_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block26_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block26_concat (Concatena (None, 12, 21, 1472) 0           conv5_block25_concat[0][0]       \n                                                                 conv5_block26_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block27_0_bn (BatchNormal (None, 12, 21, 1472) 5888        conv5_block26_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block27_0_relu (Activatio (None, 12, 21, 1472) 0           conv5_block27_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block27_1_conv (Conv2D)   (None, 12, 21, 128)  188416      conv5_block27_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block27_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block27_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block27_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block27_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block27_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block27_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block27_concat (Concatena (None, 12, 21, 1504) 0           conv5_block26_concat[0][0]       \n                                                                 conv5_block27_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block28_0_bn (BatchNormal (None, 12, 21, 1504) 6016        conv5_block27_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block28_0_relu (Activatio (None, 12, 21, 1504) 0           conv5_block28_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block28_1_conv (Conv2D)   (None, 12, 21, 128)  192512      conv5_block28_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block28_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block28_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block28_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block28_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block28_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block28_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block28_concat (Concatena (None, 12, 21, 1536) 0           conv5_block27_concat[0][0]       \n                                                                 conv5_block28_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block29_0_bn (BatchNormal (None, 12, 21, 1536) 6144        conv5_block28_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block29_0_relu (Activatio (None, 12, 21, 1536) 0           conv5_block29_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block29_1_conv (Conv2D)   (None, 12, 21, 128)  196608      conv5_block29_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block29_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block29_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block29_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block29_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block29_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block29_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block29_concat (Concatena (None, 12, 21, 1568) 0           conv5_block28_concat[0][0]       \n                                                                 conv5_block29_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block30_0_bn (BatchNormal (None, 12, 21, 1568) 6272        conv5_block29_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block30_0_relu (Activatio (None, 12, 21, 1568) 0           conv5_block30_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block30_1_conv (Conv2D)   (None, 12, 21, 128)  200704      conv5_block30_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block30_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block30_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block30_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block30_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block30_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block30_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block30_concat (Concatena (None, 12, 21, 1600) 0           conv5_block29_concat[0][0]       \n                                                                 conv5_block30_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block31_0_bn (BatchNormal (None, 12, 21, 1600) 6400        conv5_block30_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block31_0_relu (Activatio (None, 12, 21, 1600) 0           conv5_block31_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block31_1_conv (Conv2D)   (None, 12, 21, 128)  204800      conv5_block31_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block31_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block31_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block31_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block31_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block31_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block31_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block31_concat (Concatena (None, 12, 21, 1632) 0           conv5_block30_concat[0][0]       \n                                                                 conv5_block31_2_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block32_0_bn (BatchNormal (None, 12, 21, 1632) 6528        conv5_block31_concat[0][0]       \n__________________________________________________________________________________________________\nconv5_block32_0_relu (Activatio (None, 12, 21, 1632) 0           conv5_block32_0_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block32_1_conv (Conv2D)   (None, 12, 21, 128)  208896      conv5_block32_0_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block32_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block32_1_conv[0][0]       \n__________________________________________________________________________________________________\nconv5_block32_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block32_1_bn[0][0]         \n__________________________________________________________________________________________________\nconv5_block32_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block32_1_relu[0][0]       \n__________________________________________________________________________________________________\nconv5_block32_concat (Concatena (None, 12, 21, 1664) 0           conv5_block31_concat[0][0]       \n                                                                 conv5_block32_2_conv[0][0]       \n__________________________________________________________________________________________________\nbn (BatchNormalization)         (None, 12, 21, 1664) 6656        conv5_block32_concat[0][0]       \n__________________________________________________________________________________________________\nrelu (Activation)               (None, 12, 21, 1664) 0           bn[0][0]                         \n__________________________________________________________________________________________________\navg_pool (GlobalAveragePooling2 (None, 1664)         0           relu[0][0]                       \n==================================================================================================\nTotal params: 12,642,880\nTrainable params: 0\nNon-trainable params: 12,642,880\n__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYXzWaM3UTcu"
      },
      "source": [
        "# Question Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R_QqwkcUTcv"
      },
      "source": [
        "Load questions into a List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARot36DMUTcv",
        "outputId": "dde86d2b-e47c-466f-adda-9e09d1fa1cf4"
      },
      "source": [
        "import json\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "dataset_dir = os.path.join(cwd, \"VQA_Dataset\", \"train_questions_annotations.json\")\n",
        "\n",
        "# Load dataset\n",
        "with open(dataset_dir) as f:\n",
        "    dic_images = json.load(f)\n",
        "            \n",
        "# Get all questions as strings in a list\n",
        "questions = [dic['question'] for dic in dic_images.values()]\n",
        "\n",
        "# Strip '?' from questions\n",
        "questions = [s.translate(str.maketrans('', '', '?')).lower() for s in questions if not s == '']\n",
        "print(questions[12])\n",
        "\n",
        "# max_words_in_sentence = max(len(question.split(' ')) for question in questions)\n",
        "# print(max_words_in_sentence)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is there books on the bookshelf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdpNNjHfUTcv"
      },
      "source": [
        "Tokenize questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOO22cQgUTcw",
        "outputId": "dbf7031a-2054-412c-8b16-475f78a0f976"
      },
      "source": [
        "# Create Tokenizer to convert words to integers\n",
        "questions_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "questions_tokenizer.fit_on_texts(questions)\n",
        "\n",
        "questions_tokenized = questions_tokenizer.texts_to_sequences(questions)\n",
        "# each sentence into a sequence of tokens (in this case, only the 20000 most frequent)\n",
        "\n",
        "# \"hello raffaele\" -> [9, 78] \n",
        "\n",
        "questions_wtoi = questions_tokenizer.word_index # index 0 reserved for padding\n",
        "print('Total number of words:', len(questions_wtoi))\n",
        "\n",
        "print(questions_tokenized[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words: 4640\n[47, 797, 1903]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymzic0aBUTcx",
        "outputId": "b1e5dd94-e020-4e38-ad9c-0f52930bb9e1"
      },
      "source": [
        "max_question_length = max(len(sentence) for sentence in questions_tokenized)\n",
        "print('Max question length:', max_question_length)\n",
        "\n",
        "# Pad to max question sentence length\n",
        "padded_questions = pad_sequences(questions_tokenized, maxlen=max_question_length)\n",
        "\n",
        "print(\"Padded questions shape:\", padded_questions.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max question length: 21\n",
            "Padded questions shape: (58832, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo77RrxhUTcx"
      },
      "source": [
        "Load pre-trained GloVe embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcZ0PoapUTcx",
        "outputId": "8a3c55fd-fbbc-4e66-d574-cadf0f7b6ecb"
      },
      "source": [
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "#!unzip -q glove.6B.zip\n",
        "\n",
        "path_to_glove_file = os.path.join(cwd,'glove.6B\\glove.6B.100d.txt')\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found\", len(embeddings_index), \"word vectors.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6qr1hXpUTcy",
        "outputId": "ce8b1e69-9b95-4343-ef8e-ceba9af99215"
      },
      "source": [
        "num_tokens = len(questions_wtoi) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in questions_wtoi.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 4496 words (144 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTodOAuEUTcy"
      },
      "source": [
        "Create question encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd6axT5yUTcy",
        "outputId": "be07b6fc-9a04-41cf-9e1e-448a8fc9fe90"
      },
      "source": [
        "embedding_layer = tf.keras.layers.Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        "    input_length=max_question_length\n",
        ")\n",
        "\n",
        "question_encoder = tf.keras.models.Sequential()\n",
        "question_encoder.add(tf.keras.layers.Input(shape=(max_question_length,), dtype=\"int64\"))\n",
        "question_encoder.add(embedding_layer)\n",
        "question_encoder.add(tf.keras.layers.Dropout(PERC_DROP))\n",
        "question_encoder.add(tf.keras.layers.LSTM(units=FEATURES))\n",
        "question_encoder.add(tf.keras.layers.Dense(units=FEATURES))\n",
        "question_encoder.add(tf.keras.layers.LSTM(units=FEATURES))\n",
        "\n",
        "question_encoder.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 21, 100)           464100    \n_________________________________________________________________\ndropout (Dropout)            (None, 21, 100)           0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 1664)              11747840  \n=================================================================\nTotal params: 12,211,940\nTrainable params: 11,747,840\nNon-trainable params: 464,100\n_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqW-8-KqUTcz"
      },
      "source": [
        "# Create complete model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC02WONzUTc5"
      },
      "source": [
        "Load indexes for answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ova0isjGUTc6",
        "outputId": "44ce04cb-9aaf-4fbb-c883-6fc29ae34bfb",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "multiplied_features = tf.keras.layers.Multiply()([image_encoder.layers[-1].output, question_encoder.layers[-1].output])\n",
        "dense_1 = tf.keras.layers.Dense(UNITS, activation='tanh')(multiplied_features)\n",
        "drop_1 = tf.keras.layers.Dropout(PERC_DROP)(dense_1)\n",
        "out = tf.keras.layers.Dense(num_answers, activation='softmax')(drop_1)\n",
        "network = tf.keras.models.Model(inputs=[image_encoder.layers[0].input, question_encoder.layers[0].input], outputs=out)\n",
        "\n",
        "network.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block21_1_conv (Conv2D)   (None, 12, 21, 128)  163840      conv5_block21_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block21_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block21_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block21_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block21_concat (Concatena (None, 12, 21, 1312) 0           conv5_block20_concat[0][0]       \n",
            "                                                                 conv5_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block22_0_bn (BatchNormal (None, 12, 21, 1312) 5248        conv5_block21_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block22_0_relu (Activatio (None, 12, 21, 1312) 0           conv5_block22_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block22_1_conv (Conv2D)   (None, 12, 21, 128)  167936      conv5_block22_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block22_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block22_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block22_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block22_concat (Concatena (None, 12, 21, 1344) 0           conv5_block21_concat[0][0]       \n",
            "                                                                 conv5_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block23_0_bn (BatchNormal (None, 12, 21, 1344) 5376        conv5_block22_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block23_0_relu (Activatio (None, 12, 21, 1344) 0           conv5_block23_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block23_1_conv (Conv2D)   (None, 12, 21, 128)  172032      conv5_block23_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block23_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block23_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block23_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block23_concat (Concatena (None, 12, 21, 1376) 0           conv5_block22_concat[0][0]       \n",
            "                                                                 conv5_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block24_0_bn (BatchNormal (None, 12, 21, 1376) 5504        conv5_block23_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block24_0_relu (Activatio (None, 12, 21, 1376) 0           conv5_block24_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block24_1_conv (Conv2D)   (None, 12, 21, 128)  176128      conv5_block24_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block24_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block24_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block24_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block24_concat (Concatena (None, 12, 21, 1408) 0           conv5_block23_concat[0][0]       \n",
            "                                                                 conv5_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block25_0_bn (BatchNormal (None, 12, 21, 1408) 5632        conv5_block24_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block25_0_relu (Activatio (None, 12, 21, 1408) 0           conv5_block25_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block25_1_conv (Conv2D)   (None, 12, 21, 128)  180224      conv5_block25_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block25_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block25_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block25_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block25_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block25_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block25_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block25_concat (Concatena (None, 12, 21, 1440) 0           conv5_block24_concat[0][0]       \n",
            "                                                                 conv5_block25_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block26_0_bn (BatchNormal (None, 12, 21, 1440) 5760        conv5_block25_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block26_0_relu (Activatio (None, 12, 21, 1440) 0           conv5_block26_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block26_1_conv (Conv2D)   (None, 12, 21, 128)  184320      conv5_block26_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block26_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block26_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block26_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block26_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block26_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block26_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block26_concat (Concatena (None, 12, 21, 1472) 0           conv5_block25_concat[0][0]       \n",
            "                                                                 conv5_block26_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block27_0_bn (BatchNormal (None, 12, 21, 1472) 5888        conv5_block26_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block27_0_relu (Activatio (None, 12, 21, 1472) 0           conv5_block27_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block27_1_conv (Conv2D)   (None, 12, 21, 128)  188416      conv5_block27_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block27_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block27_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block27_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block27_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block27_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block27_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block27_concat (Concatena (None, 12, 21, 1504) 0           conv5_block26_concat[0][0]       \n",
            "                                                                 conv5_block27_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block28_0_bn (BatchNormal (None, 12, 21, 1504) 6016        conv5_block27_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block28_0_relu (Activatio (None, 12, 21, 1504) 0           conv5_block28_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block28_1_conv (Conv2D)   (None, 12, 21, 128)  192512      conv5_block28_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block28_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block28_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block28_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block28_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block28_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block28_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block28_concat (Concatena (None, 12, 21, 1536) 0           conv5_block27_concat[0][0]       \n",
            "                                                                 conv5_block28_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block29_0_bn (BatchNormal (None, 12, 21, 1536) 6144        conv5_block28_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block29_0_relu (Activatio (None, 12, 21, 1536) 0           conv5_block29_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block29_1_conv (Conv2D)   (None, 12, 21, 128)  196608      conv5_block29_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block29_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block29_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block29_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block29_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block29_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block29_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block29_concat (Concatena (None, 12, 21, 1568) 0           conv5_block28_concat[0][0]       \n",
            "                                                                 conv5_block29_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block30_0_bn (BatchNormal (None, 12, 21, 1568) 6272        conv5_block29_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block30_0_relu (Activatio (None, 12, 21, 1568) 0           conv5_block30_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block30_1_conv (Conv2D)   (None, 12, 21, 128)  200704      conv5_block30_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block30_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block30_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block30_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block30_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block30_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block30_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block30_concat (Concatena (None, 12, 21, 1600) 0           conv5_block29_concat[0][0]       \n",
            "                                                                 conv5_block30_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block31_0_bn (BatchNormal (None, 12, 21, 1600) 6400        conv5_block30_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block31_0_relu (Activatio (None, 12, 21, 1600) 0           conv5_block31_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block31_1_conv (Conv2D)   (None, 12, 21, 128)  204800      conv5_block31_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block31_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block31_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block31_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block31_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block31_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block31_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block31_concat (Concatena (None, 12, 21, 1632) 0           conv5_block30_concat[0][0]       \n",
            "                                                                 conv5_block31_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block32_0_bn (BatchNormal (None, 12, 21, 1632) 6528        conv5_block31_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block32_0_relu (Activatio (None, 12, 21, 1632) 0           conv5_block32_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block32_1_conv (Conv2D)   (None, 12, 21, 128)  208896      conv5_block32_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block32_1_bn (BatchNormal (None, 12, 21, 128)  512         conv5_block32_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block32_1_relu (Activatio (None, 12, 21, 128)  0           conv5_block32_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block32_2_conv (Conv2D)   (None, 12, 21, 32)   36864       conv5_block32_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block32_concat (Concatena (None, 12, 21, 1664) 0           conv5_block31_concat[0][0]       \n",
            "                                                                 conv5_block32_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 21)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bn (BatchNormalization)         (None, 12, 21, 1664) 6656        conv5_block32_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 21, 100)      464100      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "relu (Activation)               (None, 12, 21, 1664) 0           bn[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 21, 100)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 1664)         0           relu[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 1664)         11747840    dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 1664)         0           avg_pool[0][0]                   \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          426240      multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 256)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 58)           14906       dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 25,295,966\n",
            "Trainable params: 12,188,986\n",
            "Non-trainable params: 13,106,980\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpEEfu3ECEBu",
        "outputId": "5be72fad-c095-4e72-e0e8-d8202e929a28"
      },
      "source": [
        "# define the grid search parameters\n",
        "optimizer_lr = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(optimizer=optimizer)\n",
        "\n",
        "for opt in optimizer_lr:\n",
        "  optimizer = tf.keras.activations.format(opt)(learning_rate=lr)\n",
        "  param_grid = dict(optimizer=optimizer)\n",
        "  metrics = ['accuracy']\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  network.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "  import os\n",
        "  from datetime import datetime\n",
        "\n",
        "  cwd = os.getcwd()\n",
        "\n",
        "  exps_dir = os.path.join('ResultsVQA', 'basic_model')\n",
        "  if not os.path.exists(exps_dir):\n",
        "      os.makedirs(exps_dir)\n",
        "\n",
        "  now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "  exp_name = 'exp'\n",
        "\n",
        "  exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n",
        "  if not os.path.exists(exp_dir):\n",
        "      os.makedirs(exp_dir)\n",
        "      \n",
        "  callbacks = []\n",
        "\n",
        "  # Model checkpoint\n",
        "  # ----------------\n",
        "  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "  if not os.path.exists(ckpt_dir):\n",
        "      os.makedirs(ckpt_dir)\n",
        "\n",
        "  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
        "                                                    save_weights_only=True, save_best_only=True)  # False to save the model directly\n",
        "  callbacks.append(ckpt_callback)\n",
        "\n",
        "  # Early Stopping\n",
        "  # --------------\n",
        "  early_stop = True\n",
        "  if early_stop:\n",
        "      es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "      callbacks.append(es_callback)\n",
        "\n",
        "      # Callback Reduce On Plateau\n",
        "  # ------------------\n",
        "  red_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "      monitor=\"val_loss\",\n",
        "      factor=decay,\n",
        "      patience=5,\n",
        "      verbose=1,\n",
        "      mode=\"min\",\n",
        "      cooldown=0,\n",
        "      min_lr=minimum\n",
        "  )\n",
        "\n",
        "  callbacks.append(red_callback)\n",
        "\n",
        "  # ---------------------------------\n",
        "  network.fit(x=gen,\n",
        "            epochs=2,\n",
        "            steps_per_epoch=len(gen),\n",
        "            validation_data=gen_val,\n",
        "            validation_steps=len(gen_val),\n",
        "            callbacks=callbacks)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 5883 steps, validate for 1470 steps\n",
            "Epoch 1/100\n",
            "5883/5883 [==============================] - 8636s 1s/step - loss: 7.5621 - accuracy: 0.1765 - val_loss: 5.8322 - val_accuracy: 0.3092\n",
            "Epoch 2/100\n",
            "1271/5883 [=====>........................] - ETA: 1:35:35 - loss: 7.8006 - accuracy: 0.1611"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhV3bWhTs9ji"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvB8rOu5K1uM"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "from PIL import Image\n",
        "\n",
        "path = './VQA_Dataset/test_questions.json'\n",
        "max_question_length = 21\n",
        "# preprocessing_function = tf.keras.applications.vgg16.preprocess_input\n",
        "dat_dir = './VQA_Dataset/Images'\n",
        "# ckpt_dir = os.path.join(cwd, 'Checkpoints', model_name)\n",
        "# network.load_weights(os.path.join(ckpt_dir, 'basic_model_xception-weights-Jan08_22-02-28-epoch-04'))\n",
        "#os.listdir('/content/drive/MyDrive/ResultsVQA/exp_Jan06_23-07-25/ckpts')\n",
        "#network.load_weights('/content/drive/MyDrive/ResultsVQA/exp_Jan06_23-07-25/ckpts/cp_23')\n",
        "\n",
        "with open(path) as f:\n",
        "       dic_test = json.load(f)\n",
        "\n",
        "dic_test_values = dic_test.values()\n",
        "test_questions = [q['question'].lower().translate(str.maketrans('', '', string.punctuation)) for q in dic_test_values]\n",
        "\n",
        "test_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "test_tokenizer.fit_on_texts(test_questions)\n",
        "\n",
        "test_wtoi = test_tokenizer.word_index\n",
        "\n",
        "test_tokenized = test_tokenizer.texts_to_sequences(test_questions)\n",
        "\n",
        "# print(test_tokenized[45])\n",
        "# max_question_length = max(len(sentence) for sentence in test_tokenized)        \n",
        "\n",
        "results = dict()\n",
        "\n",
        "count = 0\n",
        "print('Prediction started, printing every 100 samples computed...')\n",
        "\n",
        "for question_id in dic_test.keys():\n",
        "    \n",
        "    temp_dic = dic_test[question_id]\n",
        "    # print(temp)\n",
        "    \n",
        "    # Get question text\n",
        "    question = temp_dic['question'].lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    # print(question)\n",
        "    \n",
        "    # Get related image\n",
        "    image_id = temp_dic['image_id']\n",
        "    \n",
        "    # Open image and apply preprocessing\n",
        "    img = Image.open(os.path.join(dat_dir, image_id + \".png\"))\n",
        "    img = img.convert('RGB')\n",
        "    img_arr = np.array(img)\n",
        "    img_arr = np.expand_dims(img_arr, axis=0)\n",
        "    img_arr = preprocessing_function(img_arr)\n",
        "    \n",
        "    # Tokenize question text with test tokenizer\n",
        "    #question_tokenized = test_tokenizer.texts_to_sequences([question])\n",
        "    \n",
        "    # Tokenize question text with QUESITIONS tokenizer from training phase\n",
        "    question_tokenized = questions_tokenizer.texts_to_sequences([question])\n",
        "    \n",
        "    # Pad question to correct length (21)\n",
        "    padded_question = pad_sequences(question_tokenized, maxlen=max_question_length)\n",
        "    # print(padded_question)\n",
        "    \n",
        "    # Get prediction\n",
        "    result = network.predict([img_arr, padded_question], verbose=0, batch_size=1)\n",
        "    \n",
        "    # Get index with max probability\n",
        "    result = tf.argmax(result[0])\n",
        "    result = int(result)\n",
        "    # itoa = {v: k for k, v in labels_dict.items()}\n",
        "    # answer = itoa[result]\n",
        "    # print(answer)\n",
        "    \n",
        "    # Add to results dictionary\n",
        "    results[question_id] = result\n",
        "    \n",
        "    count += 1\n",
        "    \n",
        "    if (count%100==0):\n",
        "        print(count)\n",
        "    \n",
        "    # print(question_id, question, image_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pd1NdW_4yXxC"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ8ImytbydAg"
      },
      "source": [
        "create_csv(results, results_dir='ResultsVQA')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    "
      ]
    }
  ]
}