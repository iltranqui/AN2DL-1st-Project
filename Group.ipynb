{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tensorflow_gpuenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "b4475d159700ac7f63d83dbd0a06e80f15c08c0a62544dac6ddf2e61acd99b97"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL = 0.1Ã \n",
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7267c5acef3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Defining the datasets directory\n",
    "dataset_dir = os.path.join(cwd, 'MaskDataset')\n",
    "training_dir = os.path.join(dataset_dir, 'training')\n",
    "validation_dir = os.path.join(dataset_dir, 'validation')\n",
    "test_dir = os.path.join(dataset_dir, 'test')\n",
    "\n",
    "# Create validation directory if it doesn't exist\n",
    "if not os.path.exists(validation_dir):\n",
    "    os.makedirs(validation_dir)\n",
    "\n",
    "# Loading the classes of each image into the memory\n",
    "train_classes_json_file_name = 'train_gt.json'\n",
    "train_classes_json_directory = os.path.join(dataset_dir, train_classes_json_file_name)\n",
    "\n",
    "data = {}\n",
    "\n",
    "with open(train_classes_json_directory) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "# Creating folder for each class of image for training and validation datasets\n",
    "classes = set(data.values())\n",
    "print(classes)\n",
    "\n",
    "for class_label in classes:\n",
    "    class_training_dir = os.path.join(training_dir, str(class_label))\n",
    "    class_validation_dir = os.path.join(validation_dir, str(class_label))\n",
    "    if not os.path.exists(class_training_dir):\n",
    "        os.makedirs(class_training_dir)\n",
    "    if not os.path.exists(class_validation_dir):\n",
    "        os.makedirs(class_validation_dir)\n",
    "\n",
    "# Assigning images to each training folder/class, avoiding to have the same image two times in the same folder\n",
    "for entry in os.scandir(training_dir):\n",
    "    if(entry.is_file()):\n",
    "        file_destination = os.path.join(training_dir, str(data[entry.name]), entry.name)\n",
    "        if not os.path.isfile(file_destination):\n",
    "            shutil.copy(entry.path, file_destination)\n",
    "    \n",
    "# Choosing random images to be into the validation folders, being able to repeat without cloning images\n",
    "validation_rate = VAL\n",
    "\n",
    "for class_label in classes:\n",
    "    class_training_dir = os.path.join(training_dir, str(class_label))\n",
    "    class_validation_dir = os.path.join(validation_dir, str(class_label))\n",
    "    \n",
    "    for old_entry in os.scandir(class_validation_dir):\n",
    "        os.remove(old_entry.path)\n",
    "    \n",
    "    training_entries = list(os.scandir(class_training_dir))\n",
    "    validation_size = round(len(training_entries)*validation_rate)\n",
    "    \n",
    "    for validation_entry in random.sample(training_entries, validation_size):\n",
    "        destination = os.path.join(class_validation_dir, validation_entry.name)\n",
    "        os.rename(validation_entry.path, destination)"
   ]
  },
  {
   "source": [
    "# Data augmentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_data_augmentation = True\n",
    "\n",
    "if apply_data_augmentation:\n",
    "    train_data_gen = ImageDataGenerator(\n",
    "        rotation_range=10,\n",
    "        width_shift_range=10,\n",
    "        height_shift_range=10,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='constant',\n",
    "        cval=0,\n",
    "        rescale=1/255.\n",
    "    )\n",
    "else:\n",
    "    train_data_gen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "valid_data_gen = ImageDataGenerator(rescale=1/255.)\n",
    "# test_data_gen = ImageDataGenerator(rescale=1/255.)\n",
    "\n",
    "bs = 8\n",
    "\n",
    "train_gen = train_data_gen.flow_from_directory(\n",
    "    training_dir,\n",
    "    batch_size=bs,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "valid_gen = valid_data_gen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    batch_size=bs,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# test_gen = test_data_gen.flow_from_directory(\n",
    "#     test_dir,\n",
    "#     batch_size=bs,\n",
    "#     class_mode='categorical',\n",
    "#     shuffle=True,\n",
    "#     seed=SEED\n",
    "# )\n",
    "\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "num_classes = len(classes)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: train_gen,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 256, 256, 3], [None, num_classes])\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: valid_gen,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, 256, 256, 3], [None, num_classes])\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.repeat()\n",
    "\n",
    "# test_dataset = tf.data.Dataset.from_generator(\n",
    "#     lambda: test_gen,\n",
    "#     output_types=(tf.float32, tf.float32),\n",
    "#     output_shapes=([None, 256, 256, 3], [None, num_classes])\n",
    "# )\n",
    "\n",
    "# test_dataset = test_dataset.repeat()"
   ]
  },
  {
   "source": [
    "# Building the Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d88217321718>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m conv_layer_1 = tf.keras.layers.Conv2D(\n\u001b[0m\u001b[0;32m      5\u001b[0m             \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_f\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "start_f = 8\n",
    "depth = 5\n",
    "\n",
    "conv_layer_1 = tf.keras.layers.Conv2D(\n",
    "            filters=start_f,\n",
    "            kernel_size=(4, 4),\n",
    "            strides=(1, 1),\n",
    "            padding=2,\n",
    "            input_shape=input_shape\n",
    "    ),\n",
    "\n",
    "conv_layer_2 = tf.keras.layers.Conv2D(\n",
    "            filter=start_f,\n",
    "            kernel_size=(4,4),\n",
    "            strides=(1,1),\n",
    "            padding=2,\n",
    "            input_shape=conv_layer_1\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-4de52517d9d2>, line 31)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-4de52517d9d2>\"\u001b[1;36m, line \u001b[1;32m31\u001b[0m\n\u001b[1;33m    conv_layer_2\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "for i in range(depth):\n",
    "    if i == 0:\n",
    "        input_shape = [img_h, img_w, 3]\n",
    "    else:\n",
    "        input_shape = [None]\n",
    "    \n",
    "\n",
    "    # Convolutional part\n",
    "    model.add(\n",
    "        conv_layer_1\n",
    "        conv_layer_2\n",
    "    )\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
    "    start_f *= 2\n",
    "\n",
    "# Fully connected part\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"
   ]
  }
 ]
}